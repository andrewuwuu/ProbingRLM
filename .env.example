# API Keys
OPENAI_API_KEY=your_openai_api_key_here
OPENROUTER_API_KEY=your_openrouter_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
PORTKEY_API_KEY=your_portkey_api_key_here
AI_GATEWAY_API_KEY=your_vercel_ai_gateway_key_here
AZURE_OPENAI_API_KEY=your_azure_openai_key_here

# Azure OpenAI extras (required when RLM_BACKEND=azure_openai)
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_VERSION=2024-02-01
AZURE_OPENAI_DEPLOYMENT=

# vLLM backend extra (required when RLM_BACKEND=vllm)
RLM_VLLM_BASE_URL=

# LiteLLM optional overrides
RLM_LITELLM_API_KEY=
RLM_LITELLM_API_BASE=

# Configuration
# Default backend:
# openai | openrouter | anthropic | gemini | portkey | vercel | azure_openai | litellm | vllm
RLM_BACKEND=openrouter

# Optional defaults when subagents are enabled:
# Must be set together if you want cross-provider subagent calls.
RLM_SUBAGENT_BACKEND=
RLM_SUBAGENT_MODEL=

# Optional runtime safety guards (positive integers):
# Cap RLM root-loop iterations.
RLM_MAX_ITERATIONS=
# Abort run when cumulative llm_query/llm_query_batched calls exceed this budget.
RLM_MAX_SUBAGENT_CALLS=
